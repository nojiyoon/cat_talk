import { useState, useEffect, useRef } from 'react'
import Scene from './components/Scene'
import { useSpeech } from './hooks/useSpeech'
import { useFaceLandmarker } from './hooks/useFaceLandmarker'
import { getCatResponse } from './services/aiService'
import { supabase } from './services/supabase'
import './App.css'

function App() {
  const { isListening, isSpeaking, transcript, startListening, stopListening, speak } = useSpeech()
  const { emotion, emotionScores, isReady, isModelLoading, isFaceDetected, startDetection, stopDetection } = useFaceLandmarker()
  const [lastResponse, setLastResponse] = useState('')
  // Initialize chat history from localStorage
  const [chatHistory, setChatHistory] = useState(() => {
    const saved = localStorage.getItem('cat_talk_history')
    return saved ? JSON.parse(saved) : []
  })
  const videoRef = useRef(null)
  const [cameraActive, setCameraActive] = useState(false)

  // Save chat history to localStorage whenever it changes
  useEffect(() => {
    localStorage.setItem('cat_talk_history', JSON.stringify(chatHistory))
  }, [chatHistory])

  // Initialize camera for face detection
  useEffect(() => {
    async function setupCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true })
        if (videoRef.current) {
          videoRef.current.srcObject = stream
          await videoRef.current.play()
          setCameraActive(true)

          if (isReady) {
            startDetection(videoRef.current)
          }
        }
      } catch (error) {
        console.error('Camera access denied:', error)
      }
    }

    setupCamera()

    return () => {
      stopDetection()
      if (videoRef.current?.srcObject) {
        videoRef.current.srcObject.getTracks().forEach(track => track.stop())
      }
    }
  }, [isReady, startDetection, stopDetection])

  // Handle transcript changes
  useEffect(() => {
    if (transcript && !isSpeaking) {
      handleUserInput(transcript)
    }
  }, [transcript])

  const handleUserInput = async (userMessage) => {
    if (!userMessage.trim()) return

    stopListening()

    try {
      // Send only the last 10 messages for context to save tokens
      const contextHistory = chatHistory.slice(-10)
      const response = await getCatResponse(userMessage, emotion, contextHistory)

      // Update history with user message and assistant response (keep all history)
      const newHistory = [
        ...chatHistory,
        { role: 'user', content: userMessage },
        { role: 'assistant', content: response }
      ]

      setChatHistory(newHistory)
      setLastResponse(response)
      speak(response)

      // Save to Supabase (Fire and forget)
      try {
        await supabase.from('chat_history').insert([
          { role: 'user', content: userMessage, emotion: emotion },
          { role: 'assistant', content: response, emotion: 'neutral' } // Assistant emotion could be refined later
        ])
      } catch (dbError) {
        console.error('Failed to save to DB:', dbError)
      }

    } catch (error) {
      console.error('Failed to get response:', error)
      speak('ë¯¸ì•ˆí•˜ë‹¤ëƒ¥... ë¬´ìŠ¨ ë§ì¸ì§€ ëª» ì•Œì•„ë“¤ì—ˆëƒ¥')
    }
  }

  return (
    <div className="app-container">
      {/* Video for face detection and user feedback */}
      <video
        ref={videoRef}
        className="camera-feed"
        playsInline
        muted
      />

      {/* Model Status Overlay */}
      <div className="model-status-overlay">
        {isModelLoading ? (
          <>
            <span>ğŸŸ¡</span>
            <span>ğŸ“· ëª¨ë¸ ì¤€ë¹„ ì¤‘...</span>
          </>
        ) : !isFaceDetected ? (
          <>
            <span>âšª</span>
            <span>ì–¼êµ´ì„ ë³´ì—¬ì£¼ì„¸ìš”</span>
          </>
        ) : (
          <>
            <span>ğŸŸ¢</span>
            <span>
              {emotion === 'happy' ? 'Happy' : emotion === 'sad' ? 'Sad' : 'Neutral'} /{' '}
              {emotion === 'neutral'
                ? Math.max(0, (1 - (emotionScores.smile + emotionScores.frown)) * 100).toFixed(0)
                : ((emotion === 'happy' ? emotionScores.smile : emotionScores.frown) * 100).toFixed(0)}
              %
            </span>
          </>
        )}
      </div>

      {/* 3D Scene */}
      <div className="scene-container">
        <Scene isSpeaking={isSpeaking} />
      </div>

      {/* UI Overlay */}
      <div className="ui-overlay">
        <h1 className="title">ğŸ± Talking Cat</h1>

        <div className="status-panel">
          <div className="status-item">
            <span className="status-label">Emotion:</span>
            <span className={`status-value emotion-${emotion}`}>
              {emotion === 'happy' ? 'ğŸ˜Š' : emotion === 'sad' ? 'ğŸ˜¢' : 'ğŸ˜'} {emotion}
            </span>
          </div>

          <div className="status-item">
            <span className="status-label">Camera:</span>
            <span className={`status-value ${cameraActive ? 'active' : 'inactive'}`}>
              {cameraActive ? 'âœ“' : 'âœ—'}
            </span>
          </div>
        </div>

        <button
          className={`mic-button ${isListening ? 'listening' : ''}`}
          onClick={isListening ? stopListening : startListening}
          disabled={isSpeaking}
        >
          {isListening ? 'ğŸ¤ Listening...' : isSpeaking ? 'ğŸ’¬ Speaking...' : 'ğŸ¤ Click to Talk'}
        </button>

        {transcript && (
          <div className="transcript-box">
            <strong>You:</strong> {transcript}
          </div>
        )}

        {lastResponse && (
          <div className="response-box">
            <strong>Cat:</strong> {lastResponse}
          </div>
        )}
      </div>
    </div>
  )
}

export default App
